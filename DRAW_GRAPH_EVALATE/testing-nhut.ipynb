{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-08-29T01:29:37.151352Z","iopub.status.busy":"2023-08-29T01:29:37.151006Z","iopub.status.idle":"2023-08-29T01:29:38.448054Z","shell.execute_reply":"2023-08-29T01:29:38.447086Z","shell.execute_reply.started":"2023-08-29T01:29:37.151323Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:38.455481Z","iopub.status.busy":"2023-08-29T01:29:38.453202Z","iopub.status.idle":"2023-08-29T01:29:46.059099Z","shell.execute_reply":"2023-08-29T01:29:46.058043Z","shell.execute_reply.started":"2023-08-29T01:29:38.455445Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:46.061492Z","iopub.status.busy":"2023-08-29T01:29:46.060723Z","iopub.status.idle":"2023-08-29T01:29:46.798958Z","shell.execute_reply":"2023-08-29T01:29:46.797941Z","shell.execute_reply.started":"2023-08-29T01:29:46.061454Z"},"trusted":true},"outputs":[],"source":["import glob\n","\n","\n","drow= glob.glob(r'D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\test_cnn\\drowsy\\*.*')\n","normal= glob.glob(r'D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\test_cnn\\non-drowsy\\*.*')\n","\n","\n","# drow= glob.glob(r'C:\\Users\\homin\\Downloads\\test\\drowsy\\*.*')\n","# normal= glob.glob(r'C:\\Users\\homin\\Downloads\\test\\non-drowsy\\*.*')\n","\n","\n","# drow= glob.glob(r'D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\test_vit\\drowsy\\*.*')\n","# normal= glob.glob(r'D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\test_vit\\non-drowsy\\*.*')\n","\n","\n","data = []\n","labels = []\n","\n","\n","\n","\n","for i in normal:\n","    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n","    #print(image)\n","    image=np.array(image)\n","    data.append(image)\n","    labels.append(1)  \n","for i in drow:\n","    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n","    image=np.array(image)\n","    data.append(image)\n","    labels.append(0)\n","\n","data = np.array(data)\n","labels = np.array(labels)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:46.801848Z","iopub.status.busy":"2023-08-29T01:29:46.801541Z","iopub.status.idle":"2023-08-29T01:29:46.932262Z","shell.execute_reply":"2023-08-29T01:29:46.931187Z","shell.execute_reply.started":"2023-08-29T01:29:46.801822Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\homin\\anaconda3\\envs\\py39\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow_addons as tfa\n","from keras.preprocessing import image"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:46.934447Z","iopub.status.busy":"2023-08-29T01:29:46.933805Z","iopub.status.idle":"2023-08-29T01:29:50.190672Z","shell.execute_reply":"2023-08-29T01:29:50.189617Z","shell.execute_reply.started":"2023-08-29T01:29:46.934409Z"},"trusted":true},"outputs":[],"source":["data_augmentation = keras.Sequential(\n","    [\n","        layers.Normalization(),\n","        layers.Resizing(224, 224),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(factor=0.02),\n","        layers.RandomZoom(\n","            height_factor=0.2, width_factor=0.2\n","        ),\n","    ],\n","    name=\"data_augmentation\",\n",")\n","data_augmentation.layers[0].adapt(data)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.194073Z","iopub.status.busy":"2023-08-29T01:29:50.193651Z","iopub.status.idle":"2023-08-29T01:29:50.201150Z","shell.execute_reply":"2023-08-29T01:29:50.200202Z","shell.execute_reply.started":"2023-08-29T01:29:50.194037Z"},"trusted":true},"outputs":[],"source":["def mlp(x, hidden_units, dropout_rate):\n","    for units in hidden_units:\n","        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n","        x = layers.Dropout(dropout_rate)(x)\n","    return x\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.204798Z","iopub.status.busy":"2023-08-29T01:29:50.203451Z","iopub.status.idle":"2023-08-29T01:29:50.214974Z","shell.execute_reply":"2023-08-29T01:29:50.214070Z","shell.execute_reply.started":"2023-08-29T01:29:50.204771Z"},"trusted":true},"outputs":[],"source":["class Patches(layers.Layer):\n","    def __init__(self, patch_size):\n","        super(Patches, self).__init__()\n","        self.patch_size = patch_size\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=[1, self.patch_size, self.patch_size, 1],\n","            strides=[1, self.patch_size, self.patch_size, 1],\n","            rates=[1, 1, 1, 1],\n","            padding=\"VALID\",\n","        )\n","        patch_dims = patches.shape[-1]\n","        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n","        return patches\n"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.217171Z","iopub.status.busy":"2023-08-29T01:29:50.216558Z","iopub.status.idle":"2023-08-29T01:29:50.226668Z","shell.execute_reply":"2023-08-29T01:29:50.225730Z","shell.execute_reply.started":"2023-08-29T01:29:50.217135Z"},"trusted":true},"outputs":[],"source":["learning_rate = 0.001\n","weight_decay = 0.0001\n","batch_size = 256\n","num_epochs = 1000\n","image_size = 224  # We'll resize input images to this size\n","patch_size = 16  # Size of the patches to be extract from the input images\n","num_patches = (image_size // patch_size) ** 2\n","projection_dim = 64\n","num_heads = 4\n","transformer_units = [\n","    projection_dim * 2,\n","    projection_dim,\n","]  # Size of the transformer layers\n","transformer_layers = 8\n","mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.228772Z","iopub.status.busy":"2023-08-29T01:29:50.228394Z","iopub.status.idle":"2023-08-29T01:29:50.237467Z","shell.execute_reply":"2023-08-29T01:29:50.236367Z","shell.execute_reply.started":"2023-08-29T01:29:50.228739Z"},"trusted":true},"outputs":[],"source":["class PatchEncoder(layers.Layer):\n","    def __init__(self, num_patches, projection_dim):\n","        super(PatchEncoder, self).__init__()\n","        self.num_patches = num_patches\n","        self.projection = layers.Dense(units=projection_dim)\n","        self.position_embedding = layers.Embedding(\n","            input_dim=num_patches, output_dim=projection_dim\n","        )\n","\n","    def call(self, patch):\n","        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n","        encoded = self.projection(patch) + self.position_embedding(positions)\n","        return encoded"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.241589Z","iopub.status.busy":"2023-08-29T01:29:50.241273Z","iopub.status.idle":"2023-08-29T01:29:50.251262Z","shell.execute_reply":"2023-08-29T01:29:50.250085Z","shell.execute_reply.started":"2023-08-29T01:29:50.241564Z"},"trusted":true},"outputs":[],"source":["def create_vit_classifier():\n","    inputs = layers.Input(shape=(224,224,3))\n","    # Augment data.\n","    augmented = data_augmentation(inputs)\n","    # Create patches.\n","    patches = Patches(patch_size)(augmented)\n","    # Encode patches.\n","    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n","\n","    # Create multiple layers of the Transformer block.\n","    for _ in range(transformer_layers):\n","        # Layer normalization 1.\n","        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","        # Create a multi-head attention layer.\n","        attention_output = layers.MultiHeadAttention(\n","            num_heads=num_heads, \n","            key_dim=projection_dim, \n","            dropout=0.1\n","        )(x1, x1)\n","        # Skip connection 1.\n","        x2 = layers.Add()([attention_output, encoded_patches])\n","        # Layer normalization 2.\n","        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n","        # MLP.\n","        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n","        # Skip connection 2.\n","        encoded_patches = layers.Add()([x3, x2])\n","\n","    # Create a [batch_size, projection_dim] tensor.\n","    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n","    representation = layers.Flatten()(representation)\n","    representation = layers.Dropout(0.5)(representation)\n","    # Add MLP.\n","    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n","    # Classify outputs.\n","    logits = layers.Dense(2)(features)\n","    # Create the Keras model.\n","    model = keras.Model(inputs=inputs, outputs=logits)\n","    return model\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["vgg_local = r\"E:\\HOC_TAP\\NCKH\\model\\VGG\\local_vgg_official.h5\"\n","vgg_cluster1 = r\"E:\\HOC_TAP\\NCKH\\model\\VGG\\cluster1_vgg_500.h5\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["den_local = r\"E:\\HOC_TAP\\NCKH\\model\\dense\\local2_densenet_500.h5\"\n","den_cluster1 = r\"E:\\HOC_TAP\\NCKH\\model\\dense\\cluster1_densenet_official.h5\"\n","den_cluster2 = r\"E:\\HOC_TAP\\NCKH\\model\\dense\\cluster2_densenet_official.h5\""]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["vit_local = r\"E:\\HOC_TAP\\NCKH\\model\\ViT\\local_vit_500.h5\"\n","vit_cluster1 = r\"E:\\HOC_TAP\\NCKH\\model\\ViT\\cluster1_vit_500.h5\"\n","vit_cluster2 = r\"E:\\HOC_TAP\\NCKH\\model\\ViT\\cluster2_vit_official2.h5\""]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["resnet_local = r\"E:\\HOC_TAP\\NCKH\\model\\resnet\\local_resnet_official.h5\"\n","resnet_cluster1 = r\"E:\\HOC_TAP\\NCKH\\model\\resnet\\cluster1_resnet_official.h5\""]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["lstm_local = r\"E:\\HOC_TAP\\NCKH\\model\\resnet\\local_resnet_official.h5\"\n","lstm_cluster1 = r\"E:\\HOC_TAP\\NCKH\\model\\LSTM\\cluster1_LSTM_official.h5\"\n","lstm_cluster2 = r\"E:\\HOC_TAP\\NCKH\\model\\LSTM\\cluster2_lstm_official.h5\""]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["model = tf.keras.models.load_model(den_local,compile=True)#Load model with DNN:"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:29:50.253364Z","iopub.status.busy":"2023-08-29T01:29:50.252715Z","iopub.status.idle":"2023-08-29T01:29:58.391072Z","shell.execute_reply":"2023-08-29T01:29:58.390065Z","shell.execute_reply.started":"2023-08-29T01:29:50.253328Z"},"trusted":true},"outputs":[],"source":["\n","#Load model with ViT:\n","model = create_vit_classifier()\n","model.load_weights(vit_cluster2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:33:18.600747Z","iopub.status.busy":"2023-08-29T01:33:18.600332Z","iopub.status.idle":"2023-08-29T01:33:18.616493Z","shell.execute_reply":"2023-08-29T01:33:18.610901Z","shell.execute_reply.started":"2023-08-29T01:33:18.600715Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1 1 1 ... 0 0 0]\n"]}],"source":["print(labels)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-08-29T01:34:18.477192Z","iopub.status.busy":"2023-08-29T01:34:18.476594Z","iopub.status.idle":"2023-08-29T01:34:18.913747Z","shell.execute_reply":"2023-08-29T01:34:18.912638Z","shell.execute_reply.started":"2023-08-29T01:34:18.477149Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["7/7 [==============================] - 14s 2s/step\n","[1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0\n"," 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n"," 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0\n"," 0 0 1 0 0 0 1 0 0 0 0 1 0 1 0]\n","Accuracy: 0.855000\n","Precision: 0.855320\n","Recall: 0.855000\n","F1 score: 0.854967\n"]}],"source":["y_hat = model.predict(data)\n","y_pred = np.argmax(y_hat, axis=-1)\n","y_test_label= labels.reshape(-1, 1)\n","# print(y_test_label)\n","print(y_pred)\n","# Tính accuracy: (tp + tn) / (p + n)\n","accuracy = accuracy_score(labels, y_pred)\n","print('Accuracy: %f' % accuracy)\n","# Tính precision tp / (tp + fp)\n","precision = precision_score(labels, y_pred, average='macro')\n","print('Precision: %f' % precision)\n","# Tính recall: tp / (tp + fn)\n","recall = recall_score(labels, y_pred, average='macro')\n","print('Recall: %f' % recall)\n","# Tính f1: 2 tp / (2 tp + fp + fn)\n","f1 = f1_score(labels, y_pred, average='macro')\n","print('F1 score: %f' % f1)"]},{"cell_type":"markdown","metadata":{},"source":["############################################## MY TEST################"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","def val_data_creator(config, batch_size):\n","    valid_path = r\"D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\nhut_faces2\"\n","    val_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n","        valid_path,\n","        image_size=(224, 224),\n","        batch_size=batch_size,\n","        validation_split=None,\n","        seed=123,\n","        label_mode='categorical'\n","    )\n","    # val_dataset = val_dataset.map(preprocess)  # Apply preprocessing\n","    return val_dataset\n","\n","batch_size = 16\n","result = model.evaluate(val_data_creator(None, batch_size))  # Pass None as config\n","print(result)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["4/4 [==============================] - 7s 1s/step\n","[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"," 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"]}],"source":["images= glob.glob(r'D:\\BIG_PROJECT\\face-detection-yolov8\\test_face_images\\test_vit\\non-drowsy\\*.*')\n","test = []\n","for i in images:\n","    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', target_size= (224,224))\n","    #print(image)\n","    image=np.array(image)\n","    test.append(image)\n","\n","test = np.array(test)\n","\n","result = model.predict(test)\n","pre_label = np.argmax(result, axis=-1)\n","print(pre_label)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
